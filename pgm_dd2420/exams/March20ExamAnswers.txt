March 10, 2020 PGM Exam Answers

1. (8p)
a) (4 p)

The modified running intersection property states that if any two cliques share a varible then there is exactly one path between them that has the varible included in each sepset along the path.  For a tree it is not neccessary to specify 'exactly one' since there is only one path between any to cliques.  There should not be two such paths as that would allow the same information to reach a clique twice (or more) which would double count the information.

b) (2 p)


(AB) connected to (BC) with sepset {B}
(BC) connected to (CD) with sepset {C}
(CD) connected to (DA) with sepset {D}
(DA) connected to (AB) with sepset {A}

If we for example removed the edge between (AB) and (BC) there would be no way of passing infomaton about B from AB to BC via messages.

c) (2 p)
With loopy graphs some of the common varibles between two clusters might not appear in the sepset of the edge between them as a result of enforcing the modified running intersetion property.  Thus the marginals of these varibles from each cluster belief might not be equal even if the graph is calibrated.


2. (12p)
a) (8p)

D_KL(q(Z)||p(Z|x)) = E_q[ln q(Z)] -E_q[ln p(Z|x)]  (Definition of KL divergence=
                   = E_[ln q(Z)] -E_q[ln p(Z,x)] +  E_q[ln p(x)] (factoring the joint)  
		   = E_[ln q(Z)] -E_q[ln p(Z,x)] + ln p(x)   (Since p(x) has no Z dependance it comes out of the Expectation over Z, E_q [1]=1)
ELBO = E_q[ln p(Z,x)] -E_q[ln q(Z)]  (By comparing terms)

b) (2p)
One varies the parameters of q(Z) to maximize the ELBO.  Since ln p(x) does not change when those parameters change the divergence must then be minimized.

c) (2p)
max_alpha E_q[ln p(Z,x)] -E_q[ln q(Z)]
max_alpha E_q[-.5 (Z-x)^2- .5sqrt(2pi)] -E_q[-.5 (Z-alpha)^2- .5sqrt(2pi)]
max_alpha E_q[-.5 (Z-alpha - x + alpha)^2] - .5sqrt(2pi) - [.5- .5sqrt(2pi)]
max_alpha  -.5 E_q [u^2 + 2u (x -alpha)+ (x -alpha)^2] - .5
where u=Z-alpha, We notice that E_q[u]=0 and E_q [u^2] = 1

max_alpha  -.5 E_q [(x -alpha)^2]
max_alpha  -.5 (x -alpha)^2

clearly this is maximized by chosing alpha = x which makes it =0.  All other choices would be <0.



3 (10p)
a) (4p)

The expectaion value of the function might be intractable to analytically compute. Often integrals over the distribution might be difficult while evaluating the probability density function at a given point is easy.  By sampling from the distribution and using the Monte Carlo principle we can approximate the expectation by the average of the function evaluated at the sampled values.

b) (2p)
If the random number is less than .75 we set x=0 else x=1:

samples are 1, 0, 0, 1, 0, 0


c) (2p)

We do as in b but conditioning on the x of the sample.  We only need the first 6 random numbers.  If we sample y=0 we simply reject the pair.
(1,1), (0,0) reject, (0,0) reject, (1,0) reject, (0,1), (0,0) reject.

Two samples remain (1,1) and (0, 1),

d) (2p)
The w_i is proportional to P(z=1 | x_i)

So for samples in (c) he unnormalized weights are .27 for x= 1 and  .09 for x=0
Normalized to add to one they are .75 and .25.  For (b) they end up as .3,.1, .1, .3, .1, .1.

4. (10p)

a) (2p)
The kernel is T(x_i | x_{i-1}), the probability of the next state in the Markov chain given the current state.  The next state only depends on the current state as this is a first order Markov process.  One starts at any random state and samples from the kernel repeatadly to eventualy generate a sample from the distribution of interest.  Obviously this only happens if the kernel is selected correctly.

b) (2p)

P(x')T(x|x')= P(x)T(x'|x)  is the detailed balance condition.
Sum_x P(x')T(x|x')= Sum_x P(x)T(x'|x)  Suming both sides over x.
P(x') = Sum_x P(x)T(x'|x)   Since Sum_x T(x |x')=1

That last equation is the stationary condition.

c) (4p)
Mixing refers to the kernel's ability to transition the state between regions of the domain.  Good mixing means that the states transition freely through the space without getting somewhat stuck in regions.  Burn in time referes to the number of iterations needed between starting from a random state until one can be reasonbly assured taht the samples are now from the distribution of interest and are not significantly dependent on the starting state.  If the mixing is poor the burn in time might be longer.

d) (2p)
Stationary implies that the process can converge to the desired solution.  One also would like that to be the unique solution.  For that one can require the process to be ergodic (or irreducible and aperiodic).

5. (10p)

a) (2p)
MLE is finding the y that maximizes p(x|y) where x is the data and y is the parameters of the model, for example.
MAP would be finding the y that maximizes p(y|x) which is proportioal to p(x|y)p(y).  Thus it requires some prior on y, p(y).

b) (2p)
If r is the binomial probability of x=1 and (1-r) then is the probability of x=0
, then the loglikelyhod of our data is:

ln p({x_i}|r) = sum_i ln [r^x_i (1-r)^(1-x_i)] = n1 ln r +n0 ln (1-r)

where n1 is the number of data points that have x_i=1 and n0 are the x_i=0 cases.
Maximizing this by differentiating:

0=n1/r- n0/(1-r)

n1(1-r)=n0 r

n1=(n0+n1)r

r= n1/(n0+n1)

c) (2p)

A conjugate prior has the property that when multiplied by the conditional probability of the data for some model one obtains a posteori model distributon of the same form as the prior with different parameters.

d) (2p) the beta distribution

e) (2p)  Beta(a,b) prior:  p(r) proportional to  r^(a-1) (1-r)^(b-1)

p(r|x) proportional to P(x|r)p(r) = r^n1 (1-r)^n0 r^(a-1) (1-r)^(b-1)

 = r^(n1+ a-1) (1-r)^(n0+b-1) proportional to Beta( n1+a, n0+b)


Where I do not bother with the normalization as that must work out to make it a probability in the end.


6 (10p)

a) (2p)
MCAR would be if one were checking people passing by for fever (with a policeman that forced them to stop) and your equipment malfunctioned forcing you to allow a few people to pass unchecked while you rebooted it.

Not MCAR would be if the policeman was not there and people without fever tended to run past you without being checked more than the people with a fever.

b) (2p)
If you were checking people for fever and you recorded for each person that arrived, the length of the line for being checked when they arrived also for those that did stop you record whether they had a fever or not.  In that case if the probability of them stoping only depended on the length of the line it would be MAR but not MCAR.

c) (2p)
With MAR one can  obtain unbiased estimates of our distribution parameters.  It may be computationally difficult as the likelihood may now not neatly factor, conationing expectations over the missing variables.

d)  (4p)
We could set the probabilities to some guessed values.  Then we could compute the sufficient statistice i.e. count the occurences of the relevant combinations for each table, by using either the data when the tuple had data or the expected data given the other tuple values and our guessed table values when the value was missing.  So the 'counts' would not be integers.  That would be the E step.  Those sufficient statistics would then give a new estimate of the table values, ie. the M step.  We could then repeat the two steps until convergence.


